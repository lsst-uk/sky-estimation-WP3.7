{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e952ab",
   "metadata": {},
   "source": [
    "# Photometry Example: Doing One Catalogue In Full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da4cb7",
   "metadata": {},
   "source": [
    "Owner: Aaron E. Watkins\n",
    "\n",
    "Last verified to run: 30 August 2021\n",
    "\n",
    "Verified Stack release: w_2021_33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking what version of the Stack I'm using\n",
    "! echo $HOSTNAME\n",
    "! eups list lsst_distrib -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2062008d",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "This Notebook can be run to produce Python pickle files containing photometry of injected models (which depends on the catalogue loaded in prior to image retrieval and photometry).  See codes `image_retrieval.py` and `surface_photometry.py` for documentation on the software used herein.  One must be logged into the Rubin Science Platform Notebook aspect, given the hard dependence on directories thereon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961ce06",
   "metadata": {},
   "source": [
    "### Python imports and utility function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import image_retrieval as imret # This module\n",
    "import surface_photometry as surfphot # This module\n",
    "\n",
    "from lsst.daf.persistence import Butler # NOTE: will upgrade to Gen. 3 Butler when new injection is available\n",
    "import lsst.geom as geom\n",
    "\n",
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32246fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxR(stampWidth, pa, ell):\n",
    "    '''\n",
    "    Uses the stamps themselves to determine the maximum aperture radius\n",
    "    '''\n",
    "    if (ell == 0.5):\n",
    "        if (pa == 0) | (pa == 90) | (pa == 180):\n",
    "            return 0.5*stampWidth\n",
    "        else:\n",
    "            return np.sqrt((0.5*stampWidth)**2 + (0.5*stampWidth)**2)\n",
    "    else:\n",
    "        return 0.5*stampWidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d1876",
   "metadata": {},
   "source": [
    "### Reading in table and setting up Butler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336baf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these directories to the appropriate one for the catalogue you want to use\n",
    "# This example uses the single Sersic profile models developed as part of WP 3.7.1. and 3.7.2\n",
    "catDir = '/home/lskelvin/fakes_lsstuk/input/lsstuk_lsb_sersic.fits'\n",
    "fksrepo = '/home/lskelvin/fakes_lsstuk/output/lsstuk_lsb_sersic'\n",
    "repo = '/datasets/hsc/repo/rerun/RC/w_2020_42/DM-27244'\n",
    "\n",
    "# ICL + NH dwarfs are found at the following locations\n",
    "# catDir = '/home/lskelvin/fakes_lsstuk/input/lsstuk_icl_dwarfs.fits'\n",
    "# fksrepo = '/home/lskelvin/fakes_lsstuk/output/lsstuk_icl_dwarfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35169fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCat = Table.read(catDir)\n",
    "ra = fullCat['raJ2000']\n",
    "dec = fullCat['decJ2000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "butler = Butler(repo)\n",
    "butler_fk = Butler(fksrepo)\n",
    "tract = 9615\n",
    "filters = ['g', 'r', 'i', 'z', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709f48e",
   "metadata": {},
   "source": [
    "## Pre-sky-subtraction photometry of catalogue models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7caa1aa",
   "metadata": {},
   "source": [
    "We want to loop over all of the patches with models actually present in them, so first we identify and store which patches have models.\n",
    "\n",
    "The loop that follows is based on the image retrieval mechanisms established in `image_retrieval.py`, but modified to work faster for the kind of photometry we do to produce the metrics (namely, by minimizing the number of Butler calls).  We subtract images of the CCD without models on them from the same images with the models present, and do photometry on these \"difference\" images to conduct the photometry down to infinite depth.  Using the functions in `image_retrieval.py` would slow the code down substantially, as it would need to run the Butler several times for each model, rather than once on a patch-by-patch basis like is done below.  Those specific cutout-grabbing codes are for the community at large to use and modify as it pleases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap = butler_fk.get('deepCoadd_skyMap', immediate=True)\n",
    "tractInfo = skymap[tract]    # Get world coordinate system\n",
    "\n",
    "patchIds = []\n",
    "allPos = []\n",
    "for i in range(len(ra)):\n",
    "    pos = geom.SpherePoint(ra[i], dec[i], geom.radians)\n",
    "    patchInfo = tractInfo.findPatch(pos)\n",
    "    patchIds.append(patchInfo.getIndex()) # For finding CCDs and Visit info\n",
    "    allPos.append(pos) # For checking CCD WCS pixel coordinates later\n",
    "patchIds = np.unique(patchIds, axis=0)\n",
    "print(str(len(patchIds))+' found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Numpy log errors for notebook cleanliness\n",
    "np.seterr(divide='ignore')\n",
    "np.seterr(invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f549b9",
   "metadata": {},
   "source": [
    "The main loop.  Please note that this takes a very long time to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up main dictionary; main keys are galaxy indexes\n",
    "mainDict = {}\n",
    "for i in range(len(allPos)):\n",
    "    mainDict[str(i)] = {}\n",
    "\n",
    "modelInds = list(range(len(allPos)))\n",
    "# Loop over all filters, and for each filter, loop over all patches\n",
    "for filt in filters:\n",
    "    # If re-starting from a crash, load in pickle file as main dictionary\n",
    "    # This is written to pick up where the loops left off at the crash time\n",
    "    if os.path.exists('mags.p'):\n",
    "        mainDict = pickle.load(open('mags.p', 'rb'))\n",
    "        keepInds = [int(key) for key in mainDict if filt+'Prof' in mainDict[key]]\n",
    "        if len(keepInds)==len(allPos):\n",
    "            continue\n",
    "    else:\n",
    "        # Want this remade for every filter\n",
    "        keepInds = [] # For keeping track of models that have had photometry done on them\n",
    "    print('Doing filter ',filt)\n",
    "    for pch in patchIds:\n",
    "        if len(keepInds)==len(allPos):\n",
    "            print('Found all models.  Moving to next filter....')\n",
    "            break\n",
    "        pch = str(pch[0])+','+str(pch[1])\n",
    "        print('Doing patch '+pch, end=' | ')\n",
    "        coaddId = dict(tract=tract, patch=pch, filter=\"HSC-\"+filt.upper())\n",
    "        exp = butler_fk.get('fakes_deepCoadd', coaddId, immediate=True)\n",
    "        ccds, visits = imret.getCcdVisit(exp)\n",
    "        \n",
    "        # For each CCD and Visit, checks to see if models are on it\n",
    "        # If they are, it does photometry and records it in the main dictionary\n",
    "        for j in range(len(ccds)):\n",
    "            # Stop looking once the last model is found\n",
    "            if len(keepInds)==len(allPos):\n",
    "                break\n",
    "            # First check if any of the models are in the specific visit\n",
    "            wcs = butler_fk.get('calexp_wcs', tract=tract, visit=visits[j], ccd=ccds[j])\n",
    "            for k in modelInds:\n",
    "                # Skip models already done\n",
    "                if k in keepInds:\n",
    "                    continue\n",
    "                x, y = imret.getXyCoord(wcs, allPos[k])\n",
    "                # First time a coordinate is found in the image, break this and continue\n",
    "                want = (x < 2048) & (x > 0) & (y < 4176) & (y > 0)\n",
    "                if want:\n",
    "                    brk = 0 # Allows it to move on to grabbing calexps\n",
    "                    break\n",
    "                else:\n",
    "                    brk = 1 # Otherwise, jump to next j loop\n",
    "            # If the coordinate wasn't found on any CCD or Visit, skip to the next CCD, Visit\n",
    "            if brk:\n",
    "                continue\n",
    "            # Otherwise, grab the exposures for photometry\n",
    "            fk_calexp = butler_fk.get('fakes_calexp', tract=tract, visit=visits[j], ccd=ccds[j])\n",
    "            calexp = butler_fk.get('calexp', tract=tract, visit=visits[j], ccd=ccds[j])\n",
    "            wcs = fk_calexp.getWcs()\n",
    "            magZp = calexp.getPhotoCalib().instFluxToMagnitude(1)\n",
    "            \n",
    "            # Making the \"difference\" image and doing photometry on it\n",
    "            imArr = fk_calexp.image.array - calexp.image.array\n",
    "            for k in modelInds:\n",
    "                # Skip models already done\n",
    "                if k in keepInds:\n",
    "                    continue\n",
    "                x, y = imret.getXyCoord(wcs, allPos[k])\n",
    "                # Preliminary check; coordinate must be within 1 Reff of the CCD edge\n",
    "                hlr_px = fullCat['BulgeHalfLightRadius'][k]/0.167\n",
    "                boundX = (x - hlr_px <= 0) | (x + hlr_px >= 2048)\n",
    "                boundY = (y - hlr_px <= 0) | (y + hlr_px >= 4176)\n",
    "                bounds = boundX | boundY # True means it's outside the CCD edge\n",
    "                if bounds:\n",
    "                    # Skip to next model\n",
    "                    continue\n",
    "                # Otherwise do the photometry and record that the model is done\n",
    "                # Calculate aperture size based on injected stamp dimensions\n",
    "                maxR = getMaxR(fullCat['stampWidth'][k], \n",
    "                               fullCat['pa_bulge'][k],\n",
    "                               fullCat['b_b'][k])\n",
    "                # Using 1 pixel radial bins, so the radius array is just range(len(meanI))\n",
    "                photDict = surfphot.surfBriProfs(imArr, x, y, \n",
    "                                                 180 - fullCat['pa_bulge'][k], # Note that orientation is different in calexps\n",
    "                                                 1 - fullCat['b_b'][k], \n",
    "                                                 1, maxR, magZp=magZp)\n",
    "                totFlux = np.nanmax(photDict['cog']) # Taking brightest value as magnitude\n",
    "                sbProf = photDict['meanI']\n",
    "                \n",
    "                # Finally storing the output as magnitudes\n",
    "                mainDict[str(k)][filt+'magVarMeas'] = -2.5*np.log10(totFlux) + magZp\n",
    "                mainDict[str(k)][filt+'Prof'] = -2.5*np.log10(sbProf) + magZp + 2.5*np.log10(0.168**2)\n",
    "                mainDict[str(k)][filt+'ccd'] = ccds[j] # For debugging\n",
    "                mainDict[str(k)][filt+'visit'] = visits[j] # For debugging\n",
    "                mainDict[str(k)][filt+'x'] = x # For debugging\n",
    "                mainDict[str(k)][filt+'y'] = y # For debugging\n",
    "                mainDict[str(k)][filt+'maxR'] = maxR # For debugging\n",
    "                    \n",
    "                keepInds.append(k) # If model is finished, record it here\n",
    "                pickle.dump(mainDict, open('mags.p', 'wb')) # Save progress to a pickle dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6554b60",
   "metadata": {},
   "source": [
    "## Now getting sky-subtracted magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb325d7d",
   "metadata": {},
   "source": [
    "This is the same principle as the above, but runs on the coadd images instead of individual calexps.  Here we retrieve whole patches and do photometry of all models on each patch at a time, to avoid having to retrieve stamps for every model individually.\n",
    "\n",
    "Note that this takes an additional long time to run.  Best practice is to leave these running overnight and analyze the photometry in the morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e582034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up main dictionary; main keys are galaxy indexes\n",
    "mainDict = {}\n",
    "for i in range(len(allPos)):\n",
    "    mainDict[str(i)] = {}\n",
    "    \n",
    "modelInds = list(range(len(allPos)))\n",
    "# Loop over all filters, and for each filter, loop over all patches\n",
    "for filt in filters:\n",
    "    # If re-starting from a crash, load in pickle file as main dictionary\n",
    "    if os.path.exists('coadd_mags.p'):\n",
    "        mainDict = pickle.load(open('coadd_mags.p', 'rb'))\n",
    "        keepInds = [int(key) for key in mainDict if filt+'Prof' in mainDict[key]]\n",
    "        if len(keepInds)==len(allPos):\n",
    "            continue\n",
    "    else:\n",
    "        # Want this remade for every filter\n",
    "        keepInds = [] # For keeping track of models that have completed photometry\n",
    "    print('Doing filter ',filt)\n",
    "    for pch in patchIds:\n",
    "        if len(keepInds)==len(allPos):\n",
    "            print('Found all models.  Moving to next filter....')\n",
    "            break\n",
    "        pch = str(pch[0])+','+str(pch[1])\n",
    "        print('Doing patch '+pch, end=' | ')\n",
    "        coaddId = dict(tract=tract, patch=pch, filter=\"HSC-\"+filt.upper())\n",
    "        fk_exp = butler_fk.get('fakes_deepCoadd', coaddId, immediate=True)\n",
    "        exp = butler.get('deepCoadd', coaddId, immediate=True)\n",
    "        magZp = exp.getPhotoCalib().instFluxToMagnitude(1)\n",
    "        \n",
    "        # \"Difference\" image is created here\n",
    "        imArr = fk_exp.image.array - exp.image.array\n",
    "        for k in modelInds:\n",
    "            # Skip models already done\n",
    "            if k in keepInds:\n",
    "                continue\n",
    "            # This requires a different method to derive the proper (x,y) coordinate than the visit-level calexps\n",
    "            xy = fk_exp.getWcs().skyToPixel(allPos[k]) - fk_exp.getBBox().getMin()\n",
    "            x = xy[0]\n",
    "            y = xy[1]\n",
    "            # Preliminary check; if coordinate not in image, skip loop\n",
    "            xmax = fk_exp.image.array.shape[1]\n",
    "            ymax = fk_exp.image.array.shape[0]\n",
    "            want = (x < xmax) & (x > 0) & (y < ymax) & (y > 0)\n",
    "            if not want:\n",
    "                continue\n",
    "            # Calculate maximum radius now, in exactly the same way as the previous loop\n",
    "            maxR = getMaxR(fullCat['stampWidth'][k], \n",
    "                           fullCat['pa_bulge'][k],\n",
    "                           fullCat['b_b'][k])\n",
    "            # Check now whether coordinate is within 1 Reff of the patch edge\n",
    "            # This should never be true for our two catalogues, but for the sake of robustness, we include this check.\n",
    "            hlr_px = fullCat['BulgeHalfLightRadius'][k]/0.167\n",
    "            boundX = (x - hlr_px <= 0) | (x + hlr_px >= exp.image.array.shape[1])\n",
    "            boundY = (y - hlr_px <= 0) | (y + hlr_px >= exp.image.array.shape[0])\n",
    "            bounds = boundX | boundY # True means it's outside the patch edge\n",
    "            # If not outside, do photometry on patch directly\n",
    "            if not bounds:\n",
    "                # Using 1 pixel radial bins, so the radius array is just range(len(meanI))\n",
    "                photDict = surfphot.surfBriProfs(imArr, x, y, \n",
    "                                                 fullCat['pa_bulge'][k],\n",
    "                                                 1 - fullCat['b_b'][k], \n",
    "                                                 1, maxR, magZp=magZp)\n",
    "                totFlux = np.nanmax(photDict['cog']) # Taking brightest value as magnitude\n",
    "                sbProf = photDict['meanI']\n",
    "                \n",
    "                # Storing results as magnitudes\n",
    "                mainDict[str(k)][filt+'magVarMeas'] = -2.5*np.log10(totFlux) + magZp\n",
    "                mainDict[str(k)][filt+'Prof'] = -2.5*np.log10(sbProf) + magZp + 2.5*np.log10(0.168**2)\n",
    "                mainDict[str(k)][filt+'patch'] = pch # For debugging\n",
    "                mainDict[str(k)][filt+'maxR'] = maxR # For debugging\n",
    "                    \n",
    "                keepInds.append(k) # If model has photometry, record it here\n",
    "                pickle.dump(mainDict, open('coadd_mags.p', 'wb')) # Save progress to a pickle dump\n",
    "                \n",
    "            # If stamp boundaries straddle two patches, grab a cutout of the full model\n",
    "            else:\n",
    "                fk_cut = imret.getRobustCutout(ra[k], dec[k], 2*maxR, butler_fk,\n",
    "                                               filt, 'fakes_deepCoadd',\n",
    "                                               tract, tractInfo)\n",
    "                cut = imret.getRobustCutout(ra[k], dec[k], 2*maxR, butler,\n",
    "                                            filt, 'deepCoadd',\n",
    "                                            tract, tractInfo)\n",
    "                x = fk_cut.image.array.shape[1]//2\n",
    "                y = fk_cut.image.array.shape[0]//2\n",
    "                imArr = fk_cut.image.array - cut.image.array\n",
    "                photDict = surfphot.surfBriProfs(imArr, x, y, \n",
    "                                                 fullCat['pa_bulge'][k],\n",
    "                                                 1 - fullCat['b_b'][k], \n",
    "                                                 1, maxR, magZp=magZp)\n",
    "                totFlux = np.nanmax(photDict['cog'])\n",
    "                sbProf = photDict['meanI']\n",
    "                mainDict[str(k)][filt+'magVarMeas'] = -2.5*np.log10(totFlux) + magZp\n",
    "                mainDict[str(k)][filt+'Prof'] = -2.5*np.log10(sbProf) + magZp + 2.5*np.log10(0.168**2)\n",
    "                mainDict[str(k)][filt+'patch'] = pch\n",
    "                mainDict[str(k)][filt+'maxR'] = maxR\n",
    "                    \n",
    "                keepInds.append(k)\n",
    "                pickle.dump(mainDict, open('coadd_mags.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5960903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
